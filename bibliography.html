<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>Sampling without replacement • mob</title><script src="deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><link href="deps/bootstrap-5.3.1/bootstrap.min.css" rel="stylesheet"><script src="deps/bootstrap-5.3.1/bootstrap.bundle.min.js"></script><link href="deps/font-awesome-6.5.2/css/all.min.css" rel="stylesheet"><link href="deps/font-awesome-6.5.2/css/v4-shims.min.css" rel="stylesheet"><script src="deps/headroom-0.11.0/headroom.min.js"></script><script src="deps/headroom-0.11.0/jQuery.headroom.min.js"></script><script src="deps/bootstrap-toc-1.0.1/bootstrap-toc.min.js"></script><script src="deps/clipboard.js-2.0.11/clipboard.min.js"></script><script src="deps/search-1.0.0/autocomplete.jquery.min.js"></script><script src="deps/search-1.0.0/fuse.min.js"></script><script src="deps/search-1.0.0/mark.min.js"></script><!-- pkgdown --><script src="pkgdown.js"></script><meta property="og:title" content="Sampling without replacement"></head><body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>


    <nav class="navbar navbar-expand-lg fixed-top bg-light" data-bs-theme="light" aria-label="Site navigation"><div class="container">

    <a class="navbar-brand me-2" href="index.html">mob</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="">0.0.0.9000</small>


    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto"><li class="nav-item"><a class="nav-link" href="reference/index.html">Reference</a></li>
      </ul><ul class="navbar-nav"><li class="nav-item"><form class="form-inline" role="search">
 <input class="form-control" type="search" name="search-input" id="search-input" autocomplete="off" aria-label="Search site" placeholder="Search for" data-search-index="search.json"></form></li>
      </ul></div>


  </div>
</nav><div class="container template-title-body">
<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">

      <h1>Sampling without replacement</h1>

    </div>


<div id="sampling-without-replacement" class="section level1">

<p>I have mostly researched algorithms that operate in <code>O(1)</code> space, since these are the most practical for use on a GPU. There’s a number of alternatives which use auxiliary datastructures (usually a hash set).</p>
<ul><li>
<p><a href="https://dl.acm.org/doi/pdf/10.1145/358105.893" class="external-link">“Faster Methods for Random Sampling” Vitter</a></p>
<p>This paper describes a few sequential, <code>O(1)</code> space, sampling algorithms, building up on one another.</p>
<p>The first one to be presented, <em>Algorithm S</em>, iterates over every input element, selecting it with a propability <code>k/n</code>, where <code>k</code> and <code>n</code> are the number of outputs and inputs <em>remaining</em>. This algorithm runs in <code>O(n)</code>, making it very inefficient if <code>k</code> is much smaller than <code>n</code>. Nevertheless this algorithm is frequently used as the base case for other implementations that can reduce the size of <code>n</code>.</p>
<p>The final entry and primary contribution of the paper, <em>Algorithm D</em>, runs in <code>O(k)</code> average time (where <code>k</code> is the size of the output), by drawing the size of the gaps between items to be sampled. It uses rejection sampling to draw the discrete values from a continuous approximation.</p>
</li>
<li>
<p><a href="https://dl.acm.org/doi/pdf/10.1145/214392.214402" class="external-link">“Sequential Random Sampling” Ahrens et al.</a></p>
<p>This paper proposes an algorithm based on a bernoulli sampling, where every element has a fixed probability of being selected. Generating a bernoulli sample is easy by using the geometric distribution to draw the gap sizes between selected elements.</p>
<p>On average, a bernoulli sample of the input with probability <code>k / n</code> will produce <code>k</code> elements. In practice however, it will more often than not produce a couple extra items, or too few. Given <code>q</code> the number of selected elements by the bernoulli sample, if <code>q &gt; k</code> then <code>q - k</code> elements can be removed by performing a recursive sample without replacement. If <code>q &lt; k</code>, the set of selected elements is abandoned and the process starts again. By using a probability slightly greater than <code>k / n</code>, we can err on the side of having too many elements more often than we have too few.</p>
<p>The basic algorithm (<em>Algorithm SG</em>) needs temporary storage to keep the <code>q</code> elements while it decides which ones to remove. A variant of it (<em>Algorithm SG*</em>) uses a deterministic PRNG to perform the same bernoulli sample twice: on the first run, the individual items are discarded and only the item count <code>q</code> is calculated. The second run is combined with the nested sample-without-replacement of the <code>q - k</code> elements to remove, returning items that appear only in the bernoulli sample (both samples are produced in-order, making it easy to join them).</p>
</li>
<li>
<p><a href="https://www.ittc.ku.edu/~jsv/Papers/Vit87.RandomSampling.pdf" class="external-link">“An Efficient Algorithm for Sequential Random Sampling” Vitter</a></p>
<p>Vitter has another look at <em>Algorithm D</em> from his earlier paper. The paper “reaffirm[s] the efficiency and practicality of Method D and present[s] some implementation improvements”. The paper includes benchmarks comparing its performance to Ahrens’ SG and SG* algorithms, and claims that <em>Algorithm D</em> is the fastest. It also includes a Pascal implementation of the algorithm.</p>
</li>
<li>
<p><a href="https://dl.acm.org/doi/10.1145/3157734" class="external-link">“Efficient Parallel Random Sampling—Vectorized, Cache-Efficient, and Online” Sanders et al.</a></p>
<p>A method to partition the sampling process across parallel “processors”. It proposes a divide and conquer scheme, recursively splitting the input in half and processing each half independently and in parallel. The number of output items to get from the left or from the right half can be drawn from a hypergeometric distribution. Eventually the scheme needs a non-parallel algorithm as its base case, unless one recurses until <code>k=1</code>, which can be solved trivially.</p>
<p>Rather than require coordination across the processors to communicate the variates of the hypergeometric distribution, the authors suggest using deterministic PRNG, with all processors sharing the same RNG state until their execution diverges. Once execution has diverged, the RNGs must be independent. This can be achieve by combining the raw RNG output together with the sub-range of the input being processed, using a “high-quality hash” function.</p>
<p>The paper includes a nice survey of existing methods, including a suggestion for implementating Ahrens’ <em>Algorithm SG</em> on a GPU: the initial bernoulli sample is done in parallel on the GPU, and the result is “repaired” on the CPU using <em>Algorithm S</em>.</p>
</li>
<li>
<p><a href="https://arxiv.org/pdf/2104.05091" class="external-link">“Simple, Optimal Algorithms for Random Sampling Without Replacement” Ting</a></p>
<p>Ting revisits Vitter’s <em>Algorithm D</em>, and concludes that the custom distribution that the original algorithm used is actually the beta-binomial distribution, which in turn can be implemented using only the uniform and binomial distributions. This makes implementating the algorithm much more straightforward, given that these two distributions are widely available already.</p>
</li>
<li>
<p><a href="https://www.dimacs.rutgers.edu/~graham/pubs/papers/hiddenshuffle.pdf" class="external-link">“Sequential Random Sampling Revisited: Hidden Shuffle Method” Shekelyan et al.</a></p>
<p>A completely different take on the sequential sampling without replacement problem. Rather than thinking about the size of gaps between sampled items, the algorithm simulates a Knuth Shuffle of the input, after which the first <code>k</code> elements of the input are returned. No shuffle actually takes place: the algorithm predicts what the effect of the shuffle would have been and picks the elements in a way that mirrors those effects.</p>
<p>The base case of the algorithm needs another sample without replacement, but the input size is reduced such that <code>k</code> is on the same order as <code>n</code>, making some of the more naive algorithms applicable.</p>
<p>The algorithm is very impressive and uses a couple of clever statistical tricks. An easy to read Python implementation is included in the text of the paper. The authors claim better performance than <em>Algorithm D</em> (as well as other algorithms commonly used in mainstream languages’ standard libraries).</p>
<p>The Ting and Shekelyan papers were published around the same time, and neither one cites or benchmarks against the other one. Interestingly, both papers attribute the lack of adoption of Vitter’s <em>Algorithm D</em> to the required complexity of its implementation.</p>
</li>
<li>
<p><a href="https://onlinelibrary.wiley.com/doi/pdf/10.1002/spe.3379" class="external-link">“Algorithms for generating small random samples” Cicirello</a></p>
<p>Short functions for generating 2 and 3 samples from a large input in constant time. The method generalizes to larger values of <code>k</code>, but each value of <code>k</code> requires its own function and the number of operations grows with <code>k^2</code>.</p>
<p>If we use sampling without replacement to pick the infectees of a given infector, the number of individuals to pick will likely be, on average, very small. Using this algorithm as a fast path may be beneficial.</p>
</li>
</ul></div>
<div class="section level1">
<h1 id="weighted-random-sampling">Weighted random sampling<a class="anchor" aria-label="anchor" href="#weighted-random-sampling"></a></h1>
<ul><li><a href="https://arxiv.org/pdf/2106.12270" class="external-link">“Weighted Random Sampling on GPUs” Lehmann et al</a></li>
<li><a href="https://dl.acm.org/doi/pdf/10.1145/3549934" class="external-link">“Parallel Weighted Random Sampling” Hübschle-Schneider et al</a></li>
</ul></div>
<div class="section level1">
<h1 id="compressed-bitsets">Compressed bitsets<a class="anchor" aria-label="anchor" href="#compressed-bitsets"></a></h1>
<ul><li>
<p><a href="https://arxiv.org/pdf/1402.6407" class="external-link">“Better bitmap performance with Roaring bitmaps” Chambi et al</a></p>
<p>Most exisiting bitset compression schemes use a run-length encoding, where a contiguous sequence of homogenous bits is replaced with a count of the bits. This gives good compression, but makes random access difficult.</p>
<p>Roaring bitsets use a two-level indexing data structure. The space is partitioned into chunks, each covering 64k. The top level index is an ordered array, pointing to the secondary containers. Depending on their cardinality, the containers can have two different representations. Containers with fewer than 4096 elements (TODO: or is it 4095? 4097?) are stored as an ordered array of integers. Containers with more elements than that are stored using bitmaps.</p>
</li>
<li><p><a href="https://arxiv.org/pdf/1402.4466" class="external-link">“Compressed bitmap indexes: beyond unions and intersections” Kaser et al.</a></p></li>
<li><p><a href="https://arxiv.org/pdf/1603.06549" class="external-link">“Consistently faster and smaller compressed bitmaps with Roaring” Lemire et al.</a></p></li>
<li><p><a href="https://arxiv.org/pdf/1709.07821" class="external-link">“Roaring Bitmaps: Implementation of an Optimized Software Library” Lemire et al.</a></p></li>
<li><p><a href="https://www.researchgate.net/profile/Robert-Wrembel/publication/221464250_GPU-WAH_Applying_GPUs_to_Compressing_Bitmap_Indexes_with_Word_Aligned_Hybrid/links/56cf5bf108ae059e375971b8/GPU-WAH-Applying-GPUs-to-Compressing-Bitmap-Indexes-with-Word-Aligned-Hybrid.pdf" class="external-link">“GPU-WAH: Applying GPUs to Compressing Bitmap Indexes with Word Aligned Hybrid” Andrzejewski et al.</a></p></li>
<li><p><a href="https://bibliotekanauki.pl/articles/206057.pdf" class="external-link">“GPU-PLWAH: GPU-based implementation of the PLWAH algorithm for compressing bitmaps” by Andrzejewski et al</a></p></li>
<li><p><a href="https://journalofcloudcomputing.springeropen.com/counter/pdf/10.1186/s13677-020-00191-w.pdf" class="external-link">“Parallel acceleration of CPU and GPU range queries over large data sets” Nelson et al.</a></p></li>
</ul></div>
<div class="section level1">
<h1 id="fixed-radius-near-neighbors">Fixed-radius near neighbors<a class="anchor" aria-label="anchor" href="#fixed-radius-near-neighbors"></a></h1>
<ul><li><a href="https://eprints.whiterose.ac.uk/id/eprint/153625/7/1-s2.0-S0743731519301340-main.pdf" class="external-link">“Improved GPU Near Neighbours Performance for Multi-Agent Simulations” Chisholm et al</a></li>
</ul></div>
<div class="section level1">
<h1 id="miscellaneous">Miscellaneous<a class="anchor" aria-label="anchor" href="#miscellaneous"></a></h1>
<ul><li><a href="https://www.biorxiv.org/content/10.1101/2020.04.27.063693v2.full.pdf" class="external-link">“Larger GPU-accelerated Brain Simulations with Procedural Connectivity” Knight et al.</a></li>
</ul></div>


  </main></div>


    <footer><div class="pkgdown-footer-left">
  <p>Developed by Paul Liétar.</p>
</div>

<div class="pkgdown-footer-right">
  <p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.1.3.</p>
</div>

    </footer></div>





  </body></html>

